/*
QuantCDN API

Unified API for QuantCDN Admin and QuantCloud Platform services

API version: 4.6.0
*/

// Code generated by OpenAPI Generator (https://openapi-generator.tech); DO NOT EDIT.

package quantadmingo

import (
	"bytes"
	"context"
	"io"
	"net/http"
	"net/url"
	"strings"
)


type AIInferenceAPI interface {

	/*
	ChatInference Chat inference via API Gateway (buffered responses) with multimodal support

	Sends requests to the AI API Gateway endpoint which buffers responses. Supports text, images, videos, and documents via base64 encoding.
     *
     * **Multimodal Support:**
     * - **Text**: Simple string content
     * - **Images**: Base64-encoded PNG, JPEG, GIF, WebP (up to 25MB)
     * - **Videos**: Base64-encoded MP4, MOV, WebM, etc. (up to 25MB)
     * - **Documents**: Base64-encoded PDF, DOCX, CSV, etc. (up to 25MB)
     *
     * **Supported Models:**
     * - Amazon Nova Lite, Micro, Pro (all support multimodal)
     * - Claude models (text only)
     *
     * **Usage Tips:**
     * - Use base64 encoding for images/videos < 5-10MB
     * - Place media before text prompts for best results
     * - Label multiple media files (e.g., 'Image 1:', 'Image 2:')
     * - Maximum 25MB total payload size
     *
     * **Response Patterns:**
     * - **Text-only**: Returns simple text response when no tools requested
     * - **Single tool**: Returns `toolUse` object when AI requests one tool
     * - **Multiple tools**: Returns `toolUse` array when AI requests multiple tools
     * - **Auto-execute sync**: Automatically executes tool and returns final text response
     * - **Auto-execute async**: Returns toolUse with `executionId` and `status` for polling

	@param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
	@param organisation The organisation ID
	@return AIInferenceAPIChatInferenceRequest
	*/
	ChatInference(ctx context.Context, organisation string) AIInferenceAPIChatInferenceRequest

	// ChatInferenceExecute executes the request
	//  @return ChatInference200Response
	ChatInferenceExecute(r AIInferenceAPIChatInferenceRequest) (*ChatInference200Response, *http.Response, error)

	/*
	ChatInferenceStream Chat inference via streaming endpoint (true HTTP streaming) with multimodal support

	Streams responses from the AI streaming subdomain using Server-Sent Events (SSE). Tokens are streamed in real-time as they are generated.
     *
     * **Multimodal Support:**
     * - **Text**: Simple string content
     * - **Images**: Base64-encoded PNG, JPEG, GIF, WebP (up to 25MB)
     * - **Videos**: Base64-encoded MP4, MOV, WebM, etc. (up to 25MB)
     * - **Documents**: Base64-encoded PDF, DOCX, CSV, etc. (up to 25MB)
     *
     * **Supported Models:**
     * - Amazon Nova Lite, Micro, Pro (all support multimodal)
     * - Claude models (text only)
     *
     * **Usage Tips:**
     * - Use base64 encoding for images/videos < 5-10MB
     * - Place media before text prompts for best results
     * - Label multiple media files (e.g., 'Image 1:', 'Image 2:')
     * - Maximum 25MB total payload size
     * - Streaming works with all content types (text, image, video, document)

	@param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
	@param organisation The organisation ID
	@return AIInferenceAPIChatInferenceStreamRequest
	*/
	ChatInferenceStream(ctx context.Context, organisation string) AIInferenceAPIChatInferenceStreamRequest

	// ChatInferenceStreamExecute executes the request
	//  @return string
	ChatInferenceStreamExecute(r AIInferenceAPIChatInferenceStreamRequest) (string, *http.Response, error)

	/*
	Embeddings Generate text embeddings for semantic search and RAG applications

	Generates vector embeddings for text content using embedding models. Used for semantic search, document similarity, and RAG applications.
     *
     * **Features:**
     * - Single text or batch processing (up to 100 texts)
     * - Configurable dimensions (256, 512, 1024, 8192 for Titan v2)
     * - Optional normalization to unit length
     * - Usage tracking for billing
     *
     * **Use Cases:**
     * - Semantic search across documents
     * - Similarity matching for content recommendations
     * - RAG (Retrieval-Augmented Generation) pipelines
     * - Clustering and classification
     *
     * **Available Embedding Models:**
     * - amazon.titan-embed-text-v2:0 (default, supports 256-8192 dimensions)
     * - amazon.titan-embed-text-v1:0 (1536 dimensions fixed)

	@param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
	@param organisation The organisation ID
	@return AIInferenceAPIEmbeddingsRequest
	*/
	Embeddings(ctx context.Context, organisation string) AIInferenceAPIEmbeddingsRequest

	// EmbeddingsExecute executes the request
	//  @return Embeddings200Response
	EmbeddingsExecute(r AIInferenceAPIEmbeddingsRequest) (*Embeddings200Response, *http.Response, error)

	/*
	ImageGeneration Generate images with Amazon Nova Canvas

	Generates images using Amazon Nova Canvas image generation model.
     *
     * **Region Restriction:** Nova Canvas is ONLY available in:
     * - `us-east-1` (US East, N. Virginia)
     * - `ap-northeast-1` (Asia Pacific, Tokyo)
     * - `eu-west-1` (Europe, Ireland)
     * ❌ NOT available in `ap-southeast-2` (Sydney)
     *
     * **Supported Task Types:**
     * - **TEXT_IMAGE**: Basic text-to-image generation
     * - **TEXT_IMAGE with Conditioning**: Layout-guided generation using edge detection or segmentation
     * - **COLOR_GUIDED_GENERATION**: Generate images with specific color palettes
     * - **IMAGE_VARIATION**: Create variations of existing images
     * - **INPAINTING**: Fill masked areas in images
     * - **OUTPAINTING**: Extend images beyond their borders
     * - **BACKGROUND_REMOVAL**: Remove backgrounds from images
     * - **VIRTUAL_TRY_ON**: Try on garments/objects on people
     *
     * **Quality Options:**
     * - **standard**: Faster generation, lower cost
     * - **premium**: Higher quality, slower generation
     *
     * **Timeout:** Image generation can take up to 5 minutes

	@param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
	@param organisation The organisation ID
	@return AIInferenceAPIImageGenerationRequest
	*/
	ImageGeneration(ctx context.Context, organisation string) AIInferenceAPIImageGenerationRequest

	// ImageGenerationExecute executes the request
	//  @return ImageGeneration200Response
	ImageGenerationExecute(r AIInferenceAPIImageGenerationRequest) (*ImageGeneration200Response, *http.Response, error)
}

// AIInferenceAPIService AIInferenceAPI service
type AIInferenceAPIService service

type AIInferenceAPIChatInferenceRequest struct {
	ctx context.Context
	ApiService AIInferenceAPI
	organisation string
	chatInferenceRequest *ChatInferenceRequest
}

// Chat request with optional multimodal content blocks
func (r AIInferenceAPIChatInferenceRequest) ChatInferenceRequest(chatInferenceRequest ChatInferenceRequest) AIInferenceAPIChatInferenceRequest {
	r.chatInferenceRequest = &chatInferenceRequest
	return r
}

func (r AIInferenceAPIChatInferenceRequest) Execute() (*ChatInference200Response, *http.Response, error) {
	return r.ApiService.ChatInferenceExecute(r)
}

/*
ChatInference Chat inference via API Gateway (buffered responses) with multimodal support

Sends requests to the AI API Gateway endpoint which buffers responses. Supports text, images, videos, and documents via base64 encoding.
     *
     * **Multimodal Support:**
     * - **Text**: Simple string content
     * - **Images**: Base64-encoded PNG, JPEG, GIF, WebP (up to 25MB)
     * - **Videos**: Base64-encoded MP4, MOV, WebM, etc. (up to 25MB)
     * - **Documents**: Base64-encoded PDF, DOCX, CSV, etc. (up to 25MB)
     *
     * **Supported Models:**
     * - Amazon Nova Lite, Micro, Pro (all support multimodal)
     * - Claude models (text only)
     *
     * **Usage Tips:**
     * - Use base64 encoding for images/videos < 5-10MB
     * - Place media before text prompts for best results
     * - Label multiple media files (e.g., 'Image 1:', 'Image 2:')
     * - Maximum 25MB total payload size
     *
     * **Response Patterns:**
     * - **Text-only**: Returns simple text response when no tools requested
     * - **Single tool**: Returns `toolUse` object when AI requests one tool
     * - **Multiple tools**: Returns `toolUse` array when AI requests multiple tools
     * - **Auto-execute sync**: Automatically executes tool and returns final text response
     * - **Auto-execute async**: Returns toolUse with `executionId` and `status` for polling

 @param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
 @param organisation The organisation ID
 @return AIInferenceAPIChatInferenceRequest
*/
func (a *AIInferenceAPIService) ChatInference(ctx context.Context, organisation string) AIInferenceAPIChatInferenceRequest {
	return AIInferenceAPIChatInferenceRequest{
		ApiService: a,
		ctx: ctx,
		organisation: organisation,
	}
}

// Execute executes the request
//  @return ChatInference200Response
func (a *AIInferenceAPIService) ChatInferenceExecute(r AIInferenceAPIChatInferenceRequest) (*ChatInference200Response, *http.Response, error) {
	var (
		localVarHTTPMethod   = http.MethodPost
		localVarPostBody     interface{}
		formFiles            []formFile
		localVarReturnValue  *ChatInference200Response
	)

	localBasePath, err := a.client.cfg.ServerURLWithContext(r.ctx, "AIInferenceAPIService.ChatInference")
	if err != nil {
		return localVarReturnValue, nil, &GenericOpenAPIError{error: err.Error()}
	}

	localVarPath := localBasePath + "/api/v3/organizations/{organisation}/ai/chat"
	localVarPath = strings.Replace(localVarPath, "{"+"organisation"+"}", url.PathEscape(parameterValueToString(r.organisation, "organisation")), -1)

	localVarHeaderParams := make(map[string]string)
	localVarQueryParams := url.Values{}
	localVarFormParams := url.Values{}
	if r.chatInferenceRequest == nil {
		return localVarReturnValue, nil, reportError("chatInferenceRequest is required and must be specified")
	}

	// to determine the Content-Type header
	localVarHTTPContentTypes := []string{"application/json"}

	// set Content-Type header
	localVarHTTPContentType := selectHeaderContentType(localVarHTTPContentTypes)
	if localVarHTTPContentType != "" {
		localVarHeaderParams["Content-Type"] = localVarHTTPContentType
	}

	// to determine the Accept header
	localVarHTTPHeaderAccepts := []string{"application/json"}

	// set Accept header
	localVarHTTPHeaderAccept := selectHeaderAccept(localVarHTTPHeaderAccepts)
	if localVarHTTPHeaderAccept != "" {
		localVarHeaderParams["Accept"] = localVarHTTPHeaderAccept
	}
	// body params
	localVarPostBody = r.chatInferenceRequest
	req, err := a.client.prepareRequest(r.ctx, localVarPath, localVarHTTPMethod, localVarPostBody, localVarHeaderParams, localVarQueryParams, localVarFormParams, formFiles)
	if err != nil {
		return localVarReturnValue, nil, err
	}

	localVarHTTPResponse, err := a.client.callAPI(req)
	if err != nil || localVarHTTPResponse == nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	localVarBody, err := io.ReadAll(localVarHTTPResponse.Body)
	localVarHTTPResponse.Body.Close()
	localVarHTTPResponse.Body = io.NopCloser(bytes.NewBuffer(localVarBody))
	if err != nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	if localVarHTTPResponse.StatusCode >= 300 {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: localVarHTTPResponse.Status,
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	err = a.client.decode(&localVarReturnValue, localVarBody, localVarHTTPResponse.Header.Get("Content-Type"))
	if err != nil {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: err.Error(),
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	return localVarReturnValue, localVarHTTPResponse, nil
}

type AIInferenceAPIChatInferenceStreamRequest struct {
	ctx context.Context
	ApiService AIInferenceAPI
	organisation string
	chatInferenceStreamRequest *ChatInferenceStreamRequest
}

// Chat request with optional multimodal content blocks
func (r AIInferenceAPIChatInferenceStreamRequest) ChatInferenceStreamRequest(chatInferenceStreamRequest ChatInferenceStreamRequest) AIInferenceAPIChatInferenceStreamRequest {
	r.chatInferenceStreamRequest = &chatInferenceStreamRequest
	return r
}

func (r AIInferenceAPIChatInferenceStreamRequest) Execute() (string, *http.Response, error) {
	return r.ApiService.ChatInferenceStreamExecute(r)
}

/*
ChatInferenceStream Chat inference via streaming endpoint (true HTTP streaming) with multimodal support

Streams responses from the AI streaming subdomain using Server-Sent Events (SSE). Tokens are streamed in real-time as they are generated.
     *
     * **Multimodal Support:**
     * - **Text**: Simple string content
     * - **Images**: Base64-encoded PNG, JPEG, GIF, WebP (up to 25MB)
     * - **Videos**: Base64-encoded MP4, MOV, WebM, etc. (up to 25MB)
     * - **Documents**: Base64-encoded PDF, DOCX, CSV, etc. (up to 25MB)
     *
     * **Supported Models:**
     * - Amazon Nova Lite, Micro, Pro (all support multimodal)
     * - Claude models (text only)
     *
     * **Usage Tips:**
     * - Use base64 encoding for images/videos < 5-10MB
     * - Place media before text prompts for best results
     * - Label multiple media files (e.g., 'Image 1:', 'Image 2:')
     * - Maximum 25MB total payload size
     * - Streaming works with all content types (text, image, video, document)

 @param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
 @param organisation The organisation ID
 @return AIInferenceAPIChatInferenceStreamRequest
*/
func (a *AIInferenceAPIService) ChatInferenceStream(ctx context.Context, organisation string) AIInferenceAPIChatInferenceStreamRequest {
	return AIInferenceAPIChatInferenceStreamRequest{
		ApiService: a,
		ctx: ctx,
		organisation: organisation,
	}
}

// Execute executes the request
//  @return string
func (a *AIInferenceAPIService) ChatInferenceStreamExecute(r AIInferenceAPIChatInferenceStreamRequest) (string, *http.Response, error) {
	var (
		localVarHTTPMethod   = http.MethodPost
		localVarPostBody     interface{}
		formFiles            []formFile
		localVarReturnValue  string
	)

	localBasePath, err := a.client.cfg.ServerURLWithContext(r.ctx, "AIInferenceAPIService.ChatInferenceStream")
	if err != nil {
		return localVarReturnValue, nil, &GenericOpenAPIError{error: err.Error()}
	}

	localVarPath := localBasePath + "/api/v3/organizations/{organisation}/ai/chat/stream"
	localVarPath = strings.Replace(localVarPath, "{"+"organisation"+"}", url.PathEscape(parameterValueToString(r.organisation, "organisation")), -1)

	localVarHeaderParams := make(map[string]string)
	localVarQueryParams := url.Values{}
	localVarFormParams := url.Values{}
	if r.chatInferenceStreamRequest == nil {
		return localVarReturnValue, nil, reportError("chatInferenceStreamRequest is required and must be specified")
	}

	// to determine the Content-Type header
	localVarHTTPContentTypes := []string{"application/json"}

	// set Content-Type header
	localVarHTTPContentType := selectHeaderContentType(localVarHTTPContentTypes)
	if localVarHTTPContentType != "" {
		localVarHeaderParams["Content-Type"] = localVarHTTPContentType
	}

	// to determine the Accept header
	localVarHTTPHeaderAccepts := []string{"text/event-stream"}

	// set Accept header
	localVarHTTPHeaderAccept := selectHeaderAccept(localVarHTTPHeaderAccepts)
	if localVarHTTPHeaderAccept != "" {
		localVarHeaderParams["Accept"] = localVarHTTPHeaderAccept
	}
	// body params
	localVarPostBody = r.chatInferenceStreamRequest
	req, err := a.client.prepareRequest(r.ctx, localVarPath, localVarHTTPMethod, localVarPostBody, localVarHeaderParams, localVarQueryParams, localVarFormParams, formFiles)
	if err != nil {
		return localVarReturnValue, nil, err
	}

	localVarHTTPResponse, err := a.client.callAPI(req)
	if err != nil || localVarHTTPResponse == nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	localVarBody, err := io.ReadAll(localVarHTTPResponse.Body)
	localVarHTTPResponse.Body.Close()
	localVarHTTPResponse.Body = io.NopCloser(bytes.NewBuffer(localVarBody))
	if err != nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	if localVarHTTPResponse.StatusCode >= 300 {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: localVarHTTPResponse.Status,
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	err = a.client.decode(&localVarReturnValue, localVarBody, localVarHTTPResponse.Header.Get("Content-Type"))
	if err != nil {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: err.Error(),
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	return localVarReturnValue, localVarHTTPResponse, nil
}

type AIInferenceAPIEmbeddingsRequest struct {
	ctx context.Context
	ApiService AIInferenceAPI
	organisation string
	embeddingsRequest *EmbeddingsRequest
}

// Embedding request with single or multiple texts
func (r AIInferenceAPIEmbeddingsRequest) EmbeddingsRequest(embeddingsRequest EmbeddingsRequest) AIInferenceAPIEmbeddingsRequest {
	r.embeddingsRequest = &embeddingsRequest
	return r
}

func (r AIInferenceAPIEmbeddingsRequest) Execute() (*Embeddings200Response, *http.Response, error) {
	return r.ApiService.EmbeddingsExecute(r)
}

/*
Embeddings Generate text embeddings for semantic search and RAG applications

Generates vector embeddings for text content using embedding models. Used for semantic search, document similarity, and RAG applications.
     *
     * **Features:**
     * - Single text or batch processing (up to 100 texts)
     * - Configurable dimensions (256, 512, 1024, 8192 for Titan v2)
     * - Optional normalization to unit length
     * - Usage tracking for billing
     *
     * **Use Cases:**
     * - Semantic search across documents
     * - Similarity matching for content recommendations
     * - RAG (Retrieval-Augmented Generation) pipelines
     * - Clustering and classification
     *
     * **Available Embedding Models:**
     * - amazon.titan-embed-text-v2:0 (default, supports 256-8192 dimensions)
     * - amazon.titan-embed-text-v1:0 (1536 dimensions fixed)

 @param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
 @param organisation The organisation ID
 @return AIInferenceAPIEmbeddingsRequest
*/
func (a *AIInferenceAPIService) Embeddings(ctx context.Context, organisation string) AIInferenceAPIEmbeddingsRequest {
	return AIInferenceAPIEmbeddingsRequest{
		ApiService: a,
		ctx: ctx,
		organisation: organisation,
	}
}

// Execute executes the request
//  @return Embeddings200Response
func (a *AIInferenceAPIService) EmbeddingsExecute(r AIInferenceAPIEmbeddingsRequest) (*Embeddings200Response, *http.Response, error) {
	var (
		localVarHTTPMethod   = http.MethodPost
		localVarPostBody     interface{}
		formFiles            []formFile
		localVarReturnValue  *Embeddings200Response
	)

	localBasePath, err := a.client.cfg.ServerURLWithContext(r.ctx, "AIInferenceAPIService.Embeddings")
	if err != nil {
		return localVarReturnValue, nil, &GenericOpenAPIError{error: err.Error()}
	}

	localVarPath := localBasePath + "/api/v3/organizations/{organisation}/ai/embeddings"
	localVarPath = strings.Replace(localVarPath, "{"+"organisation"+"}", url.PathEscape(parameterValueToString(r.organisation, "organisation")), -1)

	localVarHeaderParams := make(map[string]string)
	localVarQueryParams := url.Values{}
	localVarFormParams := url.Values{}
	if r.embeddingsRequest == nil {
		return localVarReturnValue, nil, reportError("embeddingsRequest is required and must be specified")
	}

	// to determine the Content-Type header
	localVarHTTPContentTypes := []string{"application/json"}

	// set Content-Type header
	localVarHTTPContentType := selectHeaderContentType(localVarHTTPContentTypes)
	if localVarHTTPContentType != "" {
		localVarHeaderParams["Content-Type"] = localVarHTTPContentType
	}

	// to determine the Accept header
	localVarHTTPHeaderAccepts := []string{"application/json"}

	// set Accept header
	localVarHTTPHeaderAccept := selectHeaderAccept(localVarHTTPHeaderAccepts)
	if localVarHTTPHeaderAccept != "" {
		localVarHeaderParams["Accept"] = localVarHTTPHeaderAccept
	}
	// body params
	localVarPostBody = r.embeddingsRequest
	req, err := a.client.prepareRequest(r.ctx, localVarPath, localVarHTTPMethod, localVarPostBody, localVarHeaderParams, localVarQueryParams, localVarFormParams, formFiles)
	if err != nil {
		return localVarReturnValue, nil, err
	}

	localVarHTTPResponse, err := a.client.callAPI(req)
	if err != nil || localVarHTTPResponse == nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	localVarBody, err := io.ReadAll(localVarHTTPResponse.Body)
	localVarHTTPResponse.Body.Close()
	localVarHTTPResponse.Body = io.NopCloser(bytes.NewBuffer(localVarBody))
	if err != nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	if localVarHTTPResponse.StatusCode >= 300 {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: localVarHTTPResponse.Status,
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	err = a.client.decode(&localVarReturnValue, localVarBody, localVarHTTPResponse.Header.Get("Content-Type"))
	if err != nil {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: err.Error(),
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	return localVarReturnValue, localVarHTTPResponse, nil
}

type AIInferenceAPIImageGenerationRequest struct {
	ctx context.Context
	ApiService AIInferenceAPI
	organisation string
	imageGenerationRequest *ImageGenerationRequest
}

// Image generation request
func (r AIInferenceAPIImageGenerationRequest) ImageGenerationRequest(imageGenerationRequest ImageGenerationRequest) AIInferenceAPIImageGenerationRequest {
	r.imageGenerationRequest = &imageGenerationRequest
	return r
}

func (r AIInferenceAPIImageGenerationRequest) Execute() (*ImageGeneration200Response, *http.Response, error) {
	return r.ApiService.ImageGenerationExecute(r)
}

/*
ImageGeneration Generate images with Amazon Nova Canvas

Generates images using Amazon Nova Canvas image generation model.
     *
     * **Region Restriction:** Nova Canvas is ONLY available in:
     * - `us-east-1` (US East, N. Virginia)
     * - `ap-northeast-1` (Asia Pacific, Tokyo)
     * - `eu-west-1` (Europe, Ireland)
     * ❌ NOT available in `ap-southeast-2` (Sydney)
     *
     * **Supported Task Types:**
     * - **TEXT_IMAGE**: Basic text-to-image generation
     * - **TEXT_IMAGE with Conditioning**: Layout-guided generation using edge detection or segmentation
     * - **COLOR_GUIDED_GENERATION**: Generate images with specific color palettes
     * - **IMAGE_VARIATION**: Create variations of existing images
     * - **INPAINTING**: Fill masked areas in images
     * - **OUTPAINTING**: Extend images beyond their borders
     * - **BACKGROUND_REMOVAL**: Remove backgrounds from images
     * - **VIRTUAL_TRY_ON**: Try on garments/objects on people
     *
     * **Quality Options:**
     * - **standard**: Faster generation, lower cost
     * - **premium**: Higher quality, slower generation
     *
     * **Timeout:** Image generation can take up to 5 minutes

 @param ctx context.Context - for authentication, logging, cancellation, deadlines, tracing, etc. Passed from http.Request or context.Background().
 @param organisation The organisation ID
 @return AIInferenceAPIImageGenerationRequest
*/
func (a *AIInferenceAPIService) ImageGeneration(ctx context.Context, organisation string) AIInferenceAPIImageGenerationRequest {
	return AIInferenceAPIImageGenerationRequest{
		ApiService: a,
		ctx: ctx,
		organisation: organisation,
	}
}

// Execute executes the request
//  @return ImageGeneration200Response
func (a *AIInferenceAPIService) ImageGenerationExecute(r AIInferenceAPIImageGenerationRequest) (*ImageGeneration200Response, *http.Response, error) {
	var (
		localVarHTTPMethod   = http.MethodPost
		localVarPostBody     interface{}
		formFiles            []formFile
		localVarReturnValue  *ImageGeneration200Response
	)

	localBasePath, err := a.client.cfg.ServerURLWithContext(r.ctx, "AIInferenceAPIService.ImageGeneration")
	if err != nil {
		return localVarReturnValue, nil, &GenericOpenAPIError{error: err.Error()}
	}

	localVarPath := localBasePath + "/api/v3/organizations/{organisation}/ai/image-generation"
	localVarPath = strings.Replace(localVarPath, "{"+"organisation"+"}", url.PathEscape(parameterValueToString(r.organisation, "organisation")), -1)

	localVarHeaderParams := make(map[string]string)
	localVarQueryParams := url.Values{}
	localVarFormParams := url.Values{}
	if r.imageGenerationRequest == nil {
		return localVarReturnValue, nil, reportError("imageGenerationRequest is required and must be specified")
	}

	// to determine the Content-Type header
	localVarHTTPContentTypes := []string{"application/json"}

	// set Content-Type header
	localVarHTTPContentType := selectHeaderContentType(localVarHTTPContentTypes)
	if localVarHTTPContentType != "" {
		localVarHeaderParams["Content-Type"] = localVarHTTPContentType
	}

	// to determine the Accept header
	localVarHTTPHeaderAccepts := []string{"application/json"}

	// set Accept header
	localVarHTTPHeaderAccept := selectHeaderAccept(localVarHTTPHeaderAccepts)
	if localVarHTTPHeaderAccept != "" {
		localVarHeaderParams["Accept"] = localVarHTTPHeaderAccept
	}
	// body params
	localVarPostBody = r.imageGenerationRequest
	req, err := a.client.prepareRequest(r.ctx, localVarPath, localVarHTTPMethod, localVarPostBody, localVarHeaderParams, localVarQueryParams, localVarFormParams, formFiles)
	if err != nil {
		return localVarReturnValue, nil, err
	}

	localVarHTTPResponse, err := a.client.callAPI(req)
	if err != nil || localVarHTTPResponse == nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	localVarBody, err := io.ReadAll(localVarHTTPResponse.Body)
	localVarHTTPResponse.Body.Close()
	localVarHTTPResponse.Body = io.NopCloser(bytes.NewBuffer(localVarBody))
	if err != nil {
		return localVarReturnValue, localVarHTTPResponse, err
	}

	if localVarHTTPResponse.StatusCode >= 300 {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: localVarHTTPResponse.Status,
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	err = a.client.decode(&localVarReturnValue, localVarBody, localVarHTTPResponse.Header.Get("Content-Type"))
	if err != nil {
		newErr := &GenericOpenAPIError{
			body:  localVarBody,
			error: err.Error(),
		}
		return localVarReturnValue, localVarHTTPResponse, newErr
	}

	return localVarReturnValue, localVarHTTPResponse, nil
}
